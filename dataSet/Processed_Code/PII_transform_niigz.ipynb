{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58c29d8f-f48a-479a-8762-f9fe2d41bab1",
   "metadata": {},
   "source": [
    "# 1. Automatic file format conversion: nii to nii.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48a73c54-a8fc-45ec-a430-b5180682ff2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion to .nii.gz completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import SimpleITK as sitk\n",
    "\n",
    "# Define input and output folders\n",
    "input_folder = \"../Raw/ForSeg\"\n",
    "output_folder = \"../Processed/PII_GZ\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Traverse all subfolders and files in the ForSeg folder\n",
    "for folder_name in os.listdir(input_folder):\n",
    "    folder_path = os.path.join(input_folder, folder_name)\n",
    "    \n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "    \n",
    "    # Process b0_img.nii and seg.nii\n",
    "    b0_img_file = os.path.join(folder_path, \"b0_img.nii\")\n",
    "    seg_file = os.path.join(folder_path, \"seg.nii\")\n",
    "    \n",
    "    if os.path.exists(b0_img_file):\n",
    "        # Read and compress to nii.gz\n",
    "        img = sitk.ReadImage(b0_img_file)\n",
    "        compressed_file_path = os.path.join(output_folder, folder_name + \"_b0_img.nii.gz\")\n",
    "        sitk.WriteImage(img, compressed_file_path)\n",
    "    \n",
    "    if os.path.exists(seg_file):\n",
    "        # Read and compress to nii.gz\n",
    "        img = sitk.ReadImage(seg_file)\n",
    "        compressed_file_path = os.path.join(output_folder, folder_name + \"_seg.nii.gz\")\n",
    "        sitk.WriteImage(img, compressed_file_path)\n",
    "\n",
    "print(\"Conversion to .nii.gz completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdb1dd5-6ea7-40f7-9d2c-fc1f4fb7a1ff",
   "metadata": {},
   "source": [
    "# 2. Extract each folder's original MRI and its segmentation result MRI into images, labels, and folders respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b328edb8-2eac-4db5-a22e-d71330cd384a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task completed! File copies and CSV log created.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "# Define the file path\n",
    "forseg_path = \"../Processed/PII_GZ\"  # Now use the compressed file path\n",
    "images_path = os.path.join(\"../Processed/PII_Full\", \"images\")\n",
    "labels_path = os.path.join(\"../Processed/PII_Full\", \"labels\")\n",
    "csv_file = os.path.join(\"../Processed/PII_Full\", \"PII_log.csv\")\n",
    "\n",
    "if not os.path.exists(forseg_path):\n",
    "    raise FileNotFoundError(f\"Path doesnt exit: {forseg_path}\")\n",
    "\n",
    "# If the images and labels folders do not exist, create them\n",
    "os.makedirs(images_path, exist_ok=True)\n",
    "os.makedirs(labels_path, exist_ok=True)\n",
    "\n",
    "# Initialize the global file number count\n",
    "global_count = 1\n",
    "\n",
    "# Open the CSV file and prepare to record the mapping between raw and separation\n",
    "with open(csv_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Original MRI File\", \"Original Seg File\", \"Dataset\", \"New Filename\"])\n",
    "    \n",
    "    # Traverse all files under PII_GZ\n",
    "    for filename in os.listdir(forseg_path):\n",
    "        if filename.endswith(\"_b0_img.nii.gz\"):\n",
    "            b0_img_file = os.path.join(forseg_path, filename)\n",
    "            seg_file = os.path.join(forseg_path, filename.replace(\"_b0_img.nii.gz\", \"_seg.nii.gz\"))\n",
    "            \n",
    "            if os.path.exists(b0_img_file) and os.path.exists(seg_file):\n",
    "                new_file_name = f\"PII_{str(global_count).zfill(3)}.nii.gz\"\n",
    "                \n",
    "                # Copy b0_img.nii.gz to the images folder\n",
    "                new_b0_img_path = os.path.join(images_path, new_file_name)\n",
    "                shutil.copy2(b0_img_file, new_b0_img_path)\n",
    "                \n",
    "                # Copy seg.nii.gz to the labels folder\n",
    "                new_seg_path = os.path.join(labels_path, new_file_name)\n",
    "                shutil.copy2(seg_file, new_seg_path)\n",
    "                \n",
    "                # Logging to CSV file\n",
    "                writer.writerow([b0_img_file, seg_file, \"images\", new_file_name])\n",
    "                writer.writerow([b0_img_file, seg_file, \"labels\", new_file_name])\n",
    "                \n",
    "                # Increment global count\n",
    "                global_count += 1\n",
    "\n",
    "print(\"Task completed! File copies and CSV log created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57d3c30-f14b-46fa-8100-8ea65833a1c2",
   "metadata": {},
   "source": [
    "# 3. Separate the datasets in images and labels into corresponding imagesTR training and imagesTs test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17328e24-5da0-4546-aaba-9f61986aa161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split into train/test sets completed and CSV logs created.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import csv\n",
    "\n",
    "# Define PII_Full path\n",
    "images_full_path = os.path.join(\"../Processed/PII_Full\", \"images\")\n",
    "labels_full_path = os.path.join(\"../Processed/PII_Full\", \"labels\")\n",
    "\n",
    "# Define the folder path after splitting\n",
    "split_base_path = \"../Processed/PII_Splite\"\n",
    "images_tr_path = os.path.join(split_base_path, \"imagesTr\")\n",
    "labels_tr_path = os.path.join(split_base_path, \"labelsTr\")\n",
    "images_ts_path = os.path.join(split_base_path, \"imagesTs\")\n",
    "labels_ts_path = os.path.join(split_base_path, \"labelsTs\")\n",
    "\n",
    "os.makedirs(images_tr_path, exist_ok=True)\n",
    "os.makedirs(labels_tr_path, exist_ok=True)\n",
    "os.makedirs(images_ts_path, exist_ok=True)\n",
    "os.makedirs(labels_ts_path, exist_ok=True)\n",
    "\n",
    "# Get all file names (assuming the file names in images and labels are the same)\n",
    "all_files = os.listdir(images_full_path)\n",
    "\n",
    "# Randomly shuffle the order of files\n",
    "random.shuffle(all_files)\n",
    "\n",
    "# Split the training set and test set into a 2:8 ratio\n",
    "split_index = int(0.8 * len(all_files))\n",
    "train_files = all_files[:split_index]\n",
    "test_files = all_files[split_index:]\n",
    "\n",
    "# Creating a CSV Log File\n",
    "train_csv = os.path.join(split_base_path, \"train_log.csv\")\n",
    "test_csv = os.path.join(split_base_path, \"test_log.csv\")\n",
    "\n",
    "# Processing the training set\n",
    "with open(train_csv, mode='w', newline='') as train_file:\n",
    "    train_writer = csv.writer(train_file)\n",
    "    train_writer.writerow([\"Original MRI Path\", \"Original Seg Path\", \"New MRI Path\", \"New Seg Path\"])\n",
    "    \n",
    "    for filename in train_files:\n",
    "        # Define the original path\n",
    "        original_img = os.path.join(images_full_path, filename)\n",
    "        original_seg = os.path.join(labels_full_path, filename)\n",
    "        \n",
    "        # Define the training set path\n",
    "        train_img = os.path.join(images_tr_path, filename)\n",
    "        train_seg = os.path.join(labels_tr_path, filename)\n",
    "        \n",
    "        # Copy the file to the training set\n",
    "        shutil.copy2(original_img, train_img)\n",
    "        shutil.copy2(original_seg, train_seg)\n",
    "        \n",
    "        # Logging to CSV\n",
    "        train_writer.writerow([original_img, original_seg, train_img, train_seg])\n",
    "\n",
    "# Processing test sets\n",
    "with open(test_csv, mode='w', newline='') as test_file:\n",
    "    test_writer = csv.writer(test_file)\n",
    "    test_writer.writerow([\"Original MRI Path\", \"Original Seg Path\", \"New MRI Path\", \"New Seg Path\"])\n",
    "    \n",
    "    for filename in test_files:\n",
    "        # Define the original path\n",
    "        original_img = os.path.join(images_full_path, filename)\n",
    "        original_seg = os.path.join(labels_full_path, filename)\n",
    "\n",
    "        # Define the testing set path\n",
    "        test_img = os.path.join(images_ts_path, filename)\n",
    "        test_seg = os.path.join(labels_ts_path, filename)\n",
    "        \n",
    "        # Copy files to the test suite\n",
    "        shutil.copy2(original_img, test_img)\n",
    "        shutil.copy2(original_seg, test_seg)\n",
    "\n",
    "        # Logging to CSV\n",
    "        test_writer.writerow([original_img, original_seg, test_img, test_seg])\n",
    "\n",
    "print(\"Data split into train/test sets completed and CSV logs created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4797f6-c9bf-451e-94cd-53cf19c34c3a",
   "metadata": {},
   "source": [
    "# 4. Create and modify the relevant json files in the final dataset to form a complete MRI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78bd9308-475d-4b42-af07-bac776bff697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.json updated successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the path to the dataset.json file\n",
    "json_file_path = \"../Processed/PII_Splite/dataset.json\"  # Relative path from the ipython notebook\n",
    "\n",
    "# Read the existing json file\n",
    "with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Update name, description, reference, license, and release date\n",
    "data['name'] = \"Placenta in Uterus\"\n",
    "data['description'] = \"Segmentation of the placenta in the uterus\"\n",
    "data['reference'] = \"Washington University in St. Louis\"\n",
    "data['licence'] = \"null\"\n",
    "data['relase'] = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Replace label 1 with \"Placenta in Uterus\"\n",
    "data['labels']['1'] = \"Placenta in Uterus\"\n",
    "\n",
    "# Define paths to the split directories\n",
    "split_base_path = \"../Processed/PII_Splite\"\n",
    "images_tr_path = os.path.join(split_base_path, \"imagesTr\")\n",
    "labels_tr_path = os.path.join(split_base_path, \"labelsTr\")\n",
    "images_ts_path = os.path.join(split_base_path, \"imagesTs\")\n",
    "labels_ts_path = os.path.join(split_base_path, \"labelsTs\")\n",
    "\n",
    "# Construct training and testing lists\n",
    "training_list = []\n",
    "testing_list = []\n",
    "\n",
    "# Iterate over training set\n",
    "for img_file in os.listdir(images_tr_path):\n",
    "    label_file = img_file  # Assuming labels and images have the same filename\n",
    "    training_list.append({\n",
    "        \"image\": f\"./imagesTr/{img_file}\",   # Ensure that image files end with .nii.gz\n",
    "        \"label\": f\"./labelsTr/{label_file}\"  # Ensure that label files end with .nii.gz\n",
    "    })\n",
    "\n",
    "# Iterate over testing set\n",
    "for img_file in os.listdir(images_ts_path):\n",
    "    testing_list.append(f\"./imagesTs/{img_file}\")  # Ensure that test image files end with .nii.gz\n",
    "\n",
    "# Update the json fields for training and testing data\n",
    "data['training'] = training_list\n",
    "data['test'] = testing_list\n",
    "\n",
    "# Update the number of training and testing samples\n",
    "data['numTraining'] = len(training_list)\n",
    "data['numTest'] = len(testing_list)\n",
    "\n",
    "# Write the updated content back to the json file\n",
    "with open(json_file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"dataset.json updated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008b44a1-6fbb-4e57-902b-e61df8604824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
